# FastVideo/FastWan Docker Image for RTX 5090 (Blackwell)
# 
# This Dockerfile is based on the EXACT working environment tested on Vast.ai
# that successfully ran FastVideo's multiprocessing executor.
#
# Tested on: December 15, 2025
# GPU: NVIDIA GeForce RTX 5090 (32GB VRAM, sm_120)
# Result: SUCCESS - multiprocessing executor works
#
# Build: docker build -f Dockerfile.fastwan -t explaindio/fastwan-worker:v1 .
# Run:   docker run --gpus all --ipc=host --shm-size=8g explaindio/fastwan-worker:v1
#
# CRITICAL: Salad Cloud must support --ipc=host for multiprocessing to work!

# Use the same base image that worked on Vast.ai
FROM pytorch/pytorch:2.6.0-cuda12.4-cudnn9-devel

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
ENV PYTHONUNBUFFERED=1
ENV TORCH_CUDA_ARCH_LIST="9.0;10.0;12.0"

# System dependencies
RUN apt-get update && apt-get install -y \
    git \
    time \
    curl \
    libgl1-mesa-glx \
    libglib2.0-0 \
    ffmpeg \
    && rm -rf /var/lib/apt/lists/*

# Upgrade pip
RUN pip install --upgrade pip

# Install PyTorch with CUDA 12.8 and sm_120 (Blackwell) support
# Matches verified configuration on Vast.ai
RUN pip install torch==2.7.1 torchvision --index-url https://download.pytorch.org/whl/cu128 --force-reinstall

# Check PyTorch version and arch list  
RUN python3 -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'Arch list: {torch.cuda.get_arch_list()}')"

# Install dependencies BEFORE FastVideo to control versions
# numpy<2.0 is CRITICAL - FastVideo/scipy breaks with numpy 2.x
RUN pip install --no-cache-dir "numpy<2.0" \
    diffusers \
    transformers \
    accelerate \
    safetensors \
    einops \
    sentencepiece \
    huggingface_hub \
    imageio \
    imageio-ffmpeg \
    opencv-python \
    ftfy \
    fastapi \
    uvicorn \
    requests \
    httpx \
    b2sdk \
    python-multipart

# Clone and install FastVideo from source
WORKDIR /workspace
RUN git clone https://github.com/hao-ai-lab/FastVideo.git && \
    cd FastVideo && \
    pip install -e .

# Apply 720p resolution patch (allows 720p generation, default is 480p max)
RUN sed -i 's/max_area = 480 \* 832/max_area = 720 \* 1280/' \
    /workspace/FastVideo/fastvideo/pipelines/stages/input_validation.py

# Install Salad HTTP Job Queue Worker
RUN set -e; \
    curl -fL \
      https://github.com/SaladTechnologies/salad-cloud-job-queue-worker/releases/download/v0.5.0/salad-http-job-queue-worker_x86_64.tar.gz \
      -o /tmp/salad-http-job-queue-worker_x86_64.tar.gz; \
    echo 'd6d4485bc63d65e4c1322a49a7f94f6acb84b663bd7c0f80baa5be2095266585  /tmp/salad-http-job-queue-worker_x86_64.tar.gz' > /tmp/worker.sha256; \
    sha256sum -c /tmp/worker.sha256; \
    tar -xzf /tmp/salad-http-job-queue-worker_x86_64.tar.gz -C /usr/local/bin; \
    rm /tmp/salad-http-job-queue-worker_x86_64.tar.gz /tmp/worker.sha256; \
    chmod +x /usr/local/bin/salad-http-job-queue-worker

# Copy our worker application
COPY worker_app_fastwan /workspace/worker_app_fastwan

# Copy entrypoint script
COPY run_fastwan_worker.sh /workspace/run_fastwan_worker.sh
RUN chmod +x /workspace/run_fastwan_worker.sh

# Create directories
RUN mkdir -p /workspace/outputs

WORKDIR /workspace

# Verify installation at build time
RUN python3 -c "import torch; print(f'PyTorch: {torch.__version__}'); \
    print(f'CUDA available: {torch.cuda.is_available()}'); \
    print(f'Arch list: {torch.cuda.get_arch_list() if hasattr(torch.cuda, \"get_arch_list\") else \"N/A\"}')"

# Default command - explicit bash execution to avoid permission/shebang issues
CMD ["/bin/bash", "/workspace/run_fastwan_worker.sh"]
