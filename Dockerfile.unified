# ==============================================================================
# MuseTalk Unified Polling Worker
# ==============================================================================
# Provider-agnostic worker that works on: Salad, OctaSpace, Vast.ai, TensorDock, Runpod
# See UNIFIED_WORKER_SPEC.md for full specification.
#
# Build Arguments:
# BASE_IMAGE: Defaults to 'explaindio/musetalk-queue-worker:progress' (Docker Hub)
#             Set to 'musetalk-base:local' if building from local Dockerfile.base in 2 steps.
# ==============================================================================

ARG BASE_IMAGE=explaindio/musetalk-worker:unified-v1
FROM ${BASE_IMAGE} AS base

# Install requests (used by unified_worker.py if needed)
# Fix dependency conflict: transformers needs huggingface-hub<1.0
RUN pip install --no-cache-dir requests "huggingface-hub<1.0"

# FORCE RE-INSTALL of PyTorch to match MuseTalk requirements (torch 2.0.1 + cu118)
# The base image has a non-standard version (2.10.0) that breaks mmcv build.
RUN pip uninstall -y torch torchvision torchaudio && \
    pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 \
    --index-url https://download.pytorch.org/whl/cu118

# Install MMLab stack (mmpose, mmcv, mmdet)
# Now that we have a standard torch version, mim should find pre-built wheels.
RUN pip install --no-cache-dir -U openmim && \
    mim install mmengine && \
    mim install "mmcv==2.0.1" && \
    mim install "mmdet==3.1.0" && \
    pip install --no-build-isolation chumpy==0.70 && \
    mim install "mmpose==1.1.0"

# Copy unified worker script
COPY unified_worker.py /app/unified_worker.py
RUN chmod +x /app/unified_worker.py

# Copy run_worker.sh (used by Vast.ai onstart)
COPY run_worker.sh /app/run_worker.sh
RUN chmod +x /app/run_worker.sh

# Copy worker app (overlaying base image version with local fixes)
COPY worker_app /app/worker_app/

# Copy fixed inference script
COPY MuseTalk/scripts/inference.py /app/scripts/inference.py
COPY MuseTalk/scripts/inference_optimized.py /app/scripts/inference_optimized.py

# Create startup script that runs both:
# 1. uvicorn (to handle /generate requests)
# 2. unified_worker.py (polling loop)
COPY <<EOF /app/start_unified.sh
#!/bin/bash
set -e

echo "Starting MuseTalk Unified Polling Worker..."
echo "Worker ID: \${SALAD_MACHINE_ID:-\${VAST_CONTAINERLABEL:-\${WORKER_ID:-unknown}}}"
echo "Provider: \${PROVIDER:-unknown}"
echo "GPU Class: \${GPU_CLASS_NAME:-unknown}"
echo "Orchestrator: \${ORCHESTRATOR_BASE_URL:-https://orch.avatargen.online}"

# Start uvicorn in background to serve /generate endpoint
uvicorn worker_app.main:app --host 0.0.0.0 --port 8000 &
UVICORN_PID=\$!

# Wait for uvicorn to be ready
echo "Waiting for uvicorn to start..."
for i in {1..30}; do
    if curl -s http://localhost:8000/health > /dev/null 2>&1; then
        echo "Uvicorn ready!"
        break
    fi
    sleep 1
done

# Start unified worker polling loop
python3 /app/unified_worker.py

# If unified_worker.py exits, kill uvicorn
kill \$UVICORN_PID 2>/dev/null || true
EOF

RUN chmod +x /app/start_unified.sh

# Default entrypoint
ENTRYPOINT ["/bin/bash", "/app/start_unified.sh"]
